{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import all of the scikit learn stuff\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "module=\"pandas\", lineno=570)\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import Text\n",
    "from nltk import FreqDist\n",
    "import os\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "def preprocessing(text):\n",
    "    # 1.Tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text)\n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    # 2.Cleaning : Remove words less than three letters & Lower capitalization & Remove Numbers\n",
    "    tokens = [word.lower() for word in tokens if len(word) >= 3  if not word.isdigit()]\n",
    "    \n",
    "    # 3.Remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend([\"n't\",\"'re\",\"i'm\",\"'ve\",'...'])\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    # 4.Lemmatization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    tokens = [lmtzr.lemmatize(word, 'v') for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "## Pos Tagging and estract\n",
    "\n",
    "def extractNouns(tokens):\n",
    "    tagged_list = pos_tag(tokens)\n",
    "    nouns_list = [t[0] for t in tagged_list if t[1] == \"NN\" or t[1] == \"NNP\"]\n",
    "    return nouns_list\n",
    "\n",
    "def extractVerbs(tokens):\n",
    "    tagged_list = pos_tag(tokens)\n",
    "    verbs_list = [t[0] for t in tagged_list if t[1] == \"VB\"]\n",
    "    return nouns_list\n",
    "\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "## Indexing\n",
    "\n",
    "## Word Indexing & Preprocessing in all the in data dir\n",
    "\n",
    "\n",
    "def indexingScripts() :\n",
    "    path_genre = './data/'\n",
    "    genre_list = os.listdir(path_genre)\n",
    "    total_freq = []\n",
    "    for i in genre_list[:]:\n",
    "        path_file = path_genre+i+'/'\n",
    "        file_list = os.listdir(path_file)\n",
    "        data = \"\"\n",
    "        for j in file_list[:]:\n",
    "            if not (j[-4:] == \".txt\") :\n",
    "                continue\n",
    "            print(\"Indexing... \",j)\n",
    "            f = open(path_file+j,'rt', encoding='utf-8')\n",
    "            data = data + f.read()\n",
    "        print(\"processing... %s\" % i)\n",
    "        preprocessed = preprocessing(data)\n",
    "        fdist=FreqDist(preprocessed)\n",
    "        freq = [i, fdist.most_common(100)]\n",
    "        total_freq += [freq]\n",
    "        print(freq,'\\n')\n",
    "    \n",
    "    return total_freq\n",
    "\n",
    "def indexingTestFiles(path_input) :\n",
    "    file_list = os.listdir(path_input) \n",
    "    ret_list = [] \n",
    "    for f in file_list[:] :\n",
    "        if not (f[-4:] == \".txt\") :\n",
    "            continue\n",
    "\n",
    "        t = open(path_input+\"/\"+f, 'rt', encoding = 'utf-8') \n",
    "        text = t.read()\n",
    "        preprocessed = preprocessing(text)\n",
    "        fdist = FreqDist(preprocessed)\n",
    "        # 100개 단어를 추린다. 이거 바꿔볼만 한듯\n",
    "        freq = [ f[:-4] , fdist.most_common(100)]\n",
    "        ret_list.append(freq)\n",
    "    return ret_list \n",
    "\n",
    "\n",
    "\n",
    "def queryProcessing(text,name) :\n",
    "    data = text\n",
    "    preprocessed = preprocessing(data)\n",
    "    fdist = FreqDist(preprocessed)\n",
    "\n",
    "    freq = [name, fdist.most_common(100)]\n",
    "    return freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each genre's file path\n",
    "actions = [] \n",
    "Adventures = []\n",
    "Animations = []\n",
    "Comedys = []\n",
    "Crimes = []\n",
    "Dramas = []\n",
    "Familys = []\n",
    "Fantasys = []\n",
    "Horrors = []\n",
    "Mysterys = []\n",
    "Romances = []\n",
    "Sci_Fis = []\n",
    "Thrillers = []\n",
    "movies = [actions,Adventures,Animations,Comedys,Crimes,Dramas,Familys,\n",
    "          Fantasys,Horrors,Mysterys,Romances,Sci_Fis,Thrillers]\n",
    "\n",
    "# save movie's contents.\n",
    "\n",
    "action_content = []\n",
    "Adventure_content = []\n",
    "Animation_content = []\n",
    "Comedy_content = []\n",
    "Crime_content = []\n",
    "Drama_content = []\n",
    "Family_content = []\n",
    "Fantasy_content = []\n",
    "Horror_content = []\n",
    "Mystery_content = []\n",
    "Romance_content = []\n",
    "Sci_Fi_content = []\n",
    "Thriller_content = []\n",
    "movies_content = [action_content,Adventure_content,Animation_content,Comedy_content,Crime_content,Drama_content,\n",
    "                 Family_content,Fantasy_content,Horror_content,Mystery_content,Romance_content,\n",
    "                    Sci_Fi_content,Thriller_content]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# save each genre's file path\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/action'):\n",
    "    for fname in files:\n",
    "        action = os.path.join(root, fname)\n",
    "        if action.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            actions.append(action)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Adventure'):\n",
    "    for fname in files:\n",
    "        Adventure = os.path.join(root, fname)\n",
    "        if Adventure.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Adventures.append(Adventure)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Animation'):\n",
    "    for fname in files:\n",
    "        Animation = os.path.join(root, fname)\n",
    "        if Animation.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Animations.append(Animation)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Comedy'):\n",
    "    for fname in files:\n",
    "        Comedy = os.path.join(root, fname)\n",
    "        if Comedy.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Comedys.append(Comedy)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Crime'):\n",
    "    for fname in files:\n",
    "        Crime = os.path.join(root, fname)\n",
    "        if Crime.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Crimes.append(Crime)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Drama'):\n",
    "    for fname in files:\n",
    "        Drama = os.path.join(root, fname)\n",
    "        if Drama.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Dramas.append(Drama)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Family'):\n",
    "    for fname in files:\n",
    "        Family = os.path.join(root, fname)\n",
    "        if Family.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Familys.append(Family)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Documents/GitHub/nlp_t2m/data/Fantasy'):\n",
    "    for fname in files:\n",
    "        Fantasy = os.path.join(root, fname)\n",
    "        if Fantasy.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Fantasys.append(Fantasy)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Horror'):\n",
    "    for fname in files:\n",
    "        Horror = os.path.join(root, fname)\n",
    "        if Horror.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Horrors.append(Horror)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Mystery'):\n",
    "    for fname in files:\n",
    "        Mystery = os.path.join(root, fname)\n",
    "        if Mystery.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Mysterys.append(Mystery)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Romance'):\n",
    "    for fname in files:\n",
    "        Romance = os.path.join(root, fname)\n",
    "        if Romance.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Romances.append(Romance)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Sci-Fi'):\n",
    "    for fname in files:\n",
    "        Sci_Fi = os.path.join(root, fname)\n",
    "        if Sci_Fi.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Sci_Fis.append(Sci_Fi)\n",
    "for root, dirs, files in os.walk('/Users/simdaebeom/Desktop/nlp_t2m-master/data/Thriller'):\n",
    "    for fname in files:\n",
    "        Thriller = os.path.join(root, fname)\n",
    "        if Thriller.endswith(\".txt\"): #'*'은 모든 값을 의미\n",
    "            Thrillers.append(Thriller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(movies)):\n",
    "    for i in range(len(movies[k])):\n",
    "        with open(movies[k][i],'r') as content_file:\n",
    "            content = content_file.read()\n",
    "            content = preprocessing(content)\n",
    "            content = \" \".join(content)\n",
    "        movies_content[k].append(content) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 장르 contents 합친다.\n",
    "action_ = \" \".join(action_content)\n",
    "Adventure_ = \" \".join(Adventure_content)\n",
    "Animation_ = \" \".join(Animation_content)\n",
    "Comedy_ = \" \".join(Comedy_content)\n",
    "Crime_ = \" \".join(Crime_content)\n",
    "Drama_ = \" \".join(Drama_content)\n",
    "Family_= \" \".join(Family_content)\n",
    "Fantasy_ = \" \".join(Fantasy_content)\n",
    "Horror_ = \" \".join(Horror_content)\n",
    "Mystery_ = \" \".join(Mystery_content)\n",
    "Romance_ = \" \".join(Romance_content)\n",
    "Sci_Fi_ = \" \".join(Sci_Fi_content)\n",
    "Thriller_ = \" \".join(Thriller_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_movie(file_name):\n",
    "    movie_content = []\n",
    "\n",
    "    movie_content.append(action_)\n",
    "    movie_content.append(Adventure_)\n",
    "    movie_content.append(Animation_)\n",
    "    movie_content.append(Comedy_)\n",
    "    movie_content.append(Crime_)\n",
    "    movie_content.append(Drama_)\n",
    "    movie_content.append(Family_)\n",
    "    movie_content.append(Fantasy_)\n",
    "    movie_content.append(Horror_)\n",
    "    movie_content.append(Mystery_)\n",
    "    movie_content.append(Romance_)\n",
    "    movie_content.append(Sci_Fi_)\n",
    "    movie_content.append(Thriller_)\n",
    "\n",
    "    movie_genre = [\"action\",\"Adventure\",\"Animation\",\"Comedy\",\"Crime\",\"Drama\",\"Family\",\"Fantasy\",\n",
    "                   \"Horror\",\"Mystery\",\"Romance\",\"Sci_Fi\",\"Thriller\"]\n",
    "    with open(file_name, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        content = preprocessing(content)\n",
    "        content = \" \".join(content)\n",
    "        movie_genre.append(os.path.basename(file_name).replace(\".txt\",\"\"))\n",
    "    movie_content.append(content)\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
    "    dtm = vectorizer.fit_transform(movie_content)  \n",
    "    \n",
    "    # Fit LSA. Use algorithm = “randomized” for large datasets\n",
    "    lsa = TruncatedSVD(2, algorithm = 'arpack')\n",
    "    dtm_lsa = lsa.fit_transform(dtm.astype(float)) \n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "    \n",
    "    #Compute document similarity using LSA components\n",
    "    similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T)\n",
    "\n",
    "    #pd.DataFrame(similarity,index=movie_genre, columns=movie_genre)\n",
    "    Similarity_movie = np.sort(similarity[len(similarity)-1])[::-1][1:4] \n",
    "\n",
    "    # 비슷한 영화 장르 index\n",
    "    first = np.argwhere(similarity[len(similarity)-1]==Similarity_movie[0])\n",
    "    second = np.argwhere(similarity[len(similarity)-1]==Similarity_movie[1])\n",
    "    third = np.argwhere(similarity[len(similarity)-1]==Similarity_movie[2])\n",
    "\n",
    "    print(movie_genre[first[0][0]]+ \"\\n\" + movie_genre[second[0][0]] +\"\\n\"+ movie_genre[third[0][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventure\n",
      "action\n",
      "Sci_Fi\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Thor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Fi\n",
      "action\n",
      "Adventure\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Alien.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama\n",
      "Comedy\n",
      "Romance\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Godfather.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comedy\n",
      "Drama\n",
      "Romance\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Hackers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mystery\n",
      "Family\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Hostage.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama\n",
      "Crime\n",
      "Family\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Inglourious-Basterds.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Fi\n",
      "action\n",
      "Adventure\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Jurassic-Park-The-Lost-World.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Fi\n",
      "action\n",
      "Adventure\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Lord-of-the-Rings-The-Two-Towers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Fi\n",
      "action\n",
      "Adventure\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Mad-Max-2-The-Road-Warrior.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Fi\n",
      "action\n",
      "Adventure\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Titanic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Fi\n",
      "action\n",
      "Adventure\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Total-Recall.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation\n",
      "Thriller\n",
      "Mystery\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/War-Horse.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romance\n",
      "Comedy\n",
      "Drama\n"
     ]
    }
   ],
   "source": [
    "compare_movie(\"/Users/simdaebeom/Desktop/nlp_t2m-master/input/Yes-Man.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
